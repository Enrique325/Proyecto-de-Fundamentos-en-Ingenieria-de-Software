-Aquí pueden pegar la información sobre los casos que hayan buscado.
▪︎Poner el nombre de empresa o persona que le sucedió.
▪︎Dar pequeño resumen de lo sucedido o el contexto donde ocurrió

Por: Juan Conde

--Problema de software Knight Capital

Contexto:
En 2013, un fracaso del programa casi llevó a la compañía de inversión Knight Capital a la bancarrota. La firma perdió medio billón de dólares en media hora porque las computadoras comenzaron a comprar y vender millones de acciones sin ningún control humano. Como resultado, los precios de las acciones de la compañía cayeron un 75% en dos días.
Información:
El problema fue causado por una actualización que no llegó a todos los sistemas informáticos, dejando algunos sistemas funcionando con código antiguo.
Para permitir la participación de sus compradores en la Retail Liquidity Program (RLP) de la bolsa de Nueva York, Knight Capital hizo numerosos cambios en sus sistemas informáticos. Entre estos cambios se incluyó el desarrollo de código nuevo para SMARS, un enrutador algorítmico de alta velocidad que enviaba las órdenes al mercado. Una de las principales funciones de SMARS era recibir las órdenes de otros componentes de la plataforma de negociación de Knight (órdenes "padre") y, en función de la liquidez disponible, enviar una o más órdenes representativas (o "hijas") a sitios externos para su ejecución.
El nuevo código RLP para SMARS pretendía reemplazar una funcionalidad conocida como Power Peg, diseñada para aumentar o disminuir el precio de las acciones de tal forma que se verificara correctamente el comportamiento de los algoritmos en entornos controlados.
A pesar de que no volvería a usarse más tras la actualización, la funcionalidad Power Peg tenía que seguir presente ya que podía ser necesitada en el despliegue. Por ello se recodificó un selector ya existente (flag) para permitir activarla. Como esta funcionalidad iba a quedar obsoleta, Knight la eliminaría en su momento poniendo este selector con el valor adecuado.
A finales de Julio del 2012, Knight desplegó su nuevo código RLP progresivamente en un número limitado de servidores SMARS durante varios días. Sin embargo, durante este despliegue uno de los técnicos de Knight olvidó copiar el código nuevo en uno de los servidores SMARS. Knight no tenía procesos de revisión para estos despliegues, así que nadie se percató de que uno de los servidores no estaba actualizado como debía.
El 1 de agosto del mismo año, Knight recibió órdenes de sus brokers para empezar a participar en el RLP. Los siete servidores con el código nuevo procesaban las órdenes correctamente, pero había uno que aún mantenía el código antiguo. Como resultado, el servidor no actualizado comenzaba a enviar sin parar órdenes hijo por cada orden padre recibida a ciertos centros de trading para su ejecución sin tener en cuenta el número de ejecuciones ya realizadas (la funcionalidad Power Peg ya no existía en el sistema SMARS).
Como el sistema SMARS no era consciente de las órdenes ya ejecutadas por ese servidor con código antiguo, Knight ejecutó 4 millones de operaciones en 154 stocks por más de 397 millones de acciones en aproximadamente 45 minutos (la continua compra/venta devaluó las acciones), lo que conllevó unas pérdidas estimadas en 460 millones de dólares.
Cuando los técnicos se dieron cuenta del error, ya era demasiado tarde. Pese a ello, Knight Capital consiguió levantar una inversión de 400 millones de dólares (entre ellos del Grupo Jefferies) para poder hacer frente al batacazo monetario.
Un error garrafal, debido a malos procedimientos de revisión y despliegue de actualizaciones en los sistemas informáticos.

*RESUMEN*
Knight Capital es una empresa de altas finanzas y trading, dedica a comprar y vender acciones de forma masiva para sus clientes

El error ocurrió al olvidar cambiar la configuración del algoritmo ya que esta se debia ejecutar en producción que en modo test. 

Lanzando millones de órdenes a diversos precios de forma muy rápida y ejecutándolos la empresa perdió más de 400 millones de dolares en menos de una hora


--Interrupción de Amazon.

Contexto:
La interrupción del servidor de Amazon en el verano de 2013 privó a muchas personas de sus datos almacenados en la nube. El accidente, inicialmente causado por un fallo al quitar mas servidores de los ordenados, lo que causo el colapso.
Información:
La interrupción del servicio que ocurrió en la región del norte de Virginia (US-EAST-1) en la mañana del 28 de febrero de 2017. El equipo de Amazon Simple Storage Service (S3) estaba depurando un problema que provocaba que el sistema de facturación de S3 progresase más lentamente de lo esperado. A las 9:37AM PST, un miembro autorizado del equipo de S3 que utiliza un libro de jugadas establecido ejecutó un comando que estaba destinado a eliminar un pequeño número de servidores para uno de los subsistemas S3 que es utilizado por el proceso de facturación de S3. Desafortunadamente, una de las entradas del comando se introdujo incorrectamente y se eliminó un conjunto más grande de servidores de lo previsto. Los servidores que se quitaron inadvertidamente admitían otros dos subsistemas S3. Uno de estos subsistemas, el subsistema de índice, administra los metadatos y la información de ubicación de todos los objetos S3 de la región. Este subsistema es necesario para atender todas las solicitudes GET, LIST, PUT y DELETE. El segundo subsistema, el subsistema de colocación, gestiona la asignación de nuevo almacenamiento y requiere que el subsistema de índices funcione correctamente para funcionar correctamente. El subsistema de colocación se utiliza durante las solicitudes PUT para asignar almacenamiento para nuevos objetos. La eliminación de una parte significativa de la capacidad hizo que cada uno de estos sistemas requirió un reinicio completo. Mientras se reiniciaban estos subsistemas, S3 no podía atender las solicitudes. Otros servicios de AWS de la región US-EAST-1 que dependen de S3 para el almacenamiento, como la consola de S3, los lanzamientos de nuevas instancias de Amazon Elastic Compute Cloud (EC2), los volúmenes de Amazon Elastic Block Store (EBS) (cuando se necesitaban datos de una instantánea de S3) y AWS Lambda también se vieron afectados mientras las API de S3 no estaban disponibles.
Los subsistemas S3 están diseñados para soportar la eliminación o falla de capacidad significativa con poco o ningún impacto en el cliente. Construimos nuestros sistemas con la suposición de que las cosas fallarán ocasionalmente, y confiamos en la capacidad de eliminar y reemplazar la capacidad como uno de nuestros procesos operativos principales. Si bien se trata de una operación en la que hemos confiado para mantener nuestros sistemas desde el lanzamiento de S3, no hemos reiniciado completamente el subsistema de índices ni el subsistema de colocación en nuestras regiones más grandes durante muchos años. S3 ha experimentado un crecimiento masivo en los últimos años y el proceso de reiniciar estos servicios y ejecutar las comprobaciones de seguridad necesarias para validar la integridad de los metadatos tomó más tiempo de lo esperado. El subsistema de índices fue el primero de los dos subsistemas afectados que debía reiniciarse. A las 12:26PM PST, el subsistema de índice había activado la capacidad suficiente para comenzar a dar servicio a las solicitudes GET, LIST y DELETE de S3. A las 1:18PM PST, el subsistema de índice se recuperó por completo y las API GET, LIST y DELETE funcionaban normalmente. La API de S3 PUT también requería el subsistema de colocación. El subsistema de colocación comenzó la recuperación cuando el subsistema de índice era funcional y terminó la recuperación a 1:54PM PST. En este punto, S3 estaba operando normalmente. Otros servicios de AWS que se vieron afectados por este evento comenzaron a recuperarse. Algunos de estos servicios habían acumulado un atraso de trabajo durante la interrupción del S3 y requerían tiempo adicional para recuperarse por completo.
Estamos haciendo varios cambios como resultado de este evento operativo. Si bien la eliminación de la capacidad es una práctica operativa clave, en este caso, la herramienta utilizada permitía eliminar demasiada capacidad demasiado rápido. Hemos modificado esta herramienta para eliminar la capacidad más lentamente y las salvaguardias añadidas para evitar que se elimine la capacidad cuando tomará cualquier subsistema por debajo de su nivel mínimo de capacidad requerido. Esto evitará que una entrada incorrecta active un evento similar en el futuro. También estamos auditando nuestras otras herramientas operativas para garantizar que tenemos controles de seguridad similares. También realizaremos cambios para mejorar el tiempo de recuperación de los subsistemas clave de S3. Empleamos múltiples técnicas para permitir que nuestros servicios se recuperen de cualquier fallo rápidamente. Uno de los más importantes implica dividir los servicios en pequeñas particiones que llamamos celdas. Al factorizar los servicios en células, los equipos de ingeniería pueden evaluar y probar a fondo los procesos de recuperación incluso del servicio o subsistema más grande. A medida que S3 ha escalado, el equipo ha hecho un trabajo considerable para refactorizar partes del servicio en celdas más pequeñas para reducir el radio de explosión y mejorar la recuperación. Durante este evento, el tiempo de recuperación del subsistema de índice todavía tardó más de lo que esperábamos. El equipo de S3 había planeado una mayor partición del subsistema de índices a finales de este año. Estamos reordenando ese trabajo para comenzar inmediatamente.
Desde el comienzo de este evento hasta las 11:37AM PST, no pudimos actualizar el estado de los servicios individuales en el panel de estado de AWS Service (SHD) debido a una dependencia que la consola de administración de SHD tiene en Amazon S3. En su lugar, utilizamos la fuente de TWITTER de AWS (AWScloud) y el texto del banner SHD para comunicar el estado hasta que pudimos actualizar el estado de los servicios individuales en el SHD. Entendemos que el SHD proporciona una visibilidad importante a nuestros clientes durante los eventos operativos y hemos cambiado la consola de administración de SHD para que se ejecute en varias regiones de AWS.
